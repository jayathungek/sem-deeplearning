{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aedea4b",
   "metadata": {},
   "source": [
    "# 2. Pandas\n",
    "\n",
    "In this exercise, we will be looking at pandas, a Python library that provides many useful tools for loading, displaying, and cleaning data. Please have a look at the [official Pandas documentation](https://pandas.pydata.org/docs/reference/index.html) to learn more about any of the functions you encouter in this notebook.\n",
    "\n",
    "To aid us in showing off the functionality of this library, we will be looking at the MetObjects dataset, which comes courtesy of the [Metropolitan Museum of Art in New York](https://www.metmuseum.org/). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08bba43",
   "metadata": {},
   "source": [
    "## 2.1 Downloading the dataset\n",
    "You can use a leading `!` in a line of Jupyter notebook code to specify that the rest of the line should be interpreted as a shell command. This is convenient for modifying files or running scripts that live on your filesystem without having to switch between the browser and terminal. Let's use this syntax to create a directory for the Met Museum dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab33955",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/metmuseum\"   # Normal python code\n",
    "\n",
    "\n",
    "# Jupyter notebook \"magic\" lines prepended with a ! character\n",
    "!mkdir -p $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e67be",
   "metadata": {},
   "source": [
    "Now download the CSV file from the Week 2 folder on Brightspace and place it into the directory you have just created. Once that's done, we can define a string that points to our CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset CSV\n",
    "MET_DATA_CSV = f\"{data_dir}/MetObjects.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c44439d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Module imports and plot settings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None \n",
    "plt.rcParams[\"figure.figsize\"] = [8, 7]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf7aa6",
   "metadata": {},
   "source": [
    "## 2.2 The DataFrame\n",
    "\n",
    "The DataFrame is the central data structure provided by Pandas, and it is this structure that we need to interrogate when we want to ask questions about our data. You can think of a DataFrame as a table with rows of records and columns that describe the fields of those records. Pandas provides built in functions for loading text files and automatically puts their contents into a DataFrame. The dataset we just downloaded (`MetObjects.csv`) is a CSV (comma separated value) file, so we need to use the `load_csv` function provided by Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf93f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into a DataFrame\n",
    "# The `sep` argument is the delimiter character, which \n",
    "# tells pandas how to separate columns. Be careful: always inspect your CSV \n",
    "# files to check what the delimiter character is: sometimes people use\n",
    "# tabs (\\t). Make a note of what the delimiter is and pass it into \n",
    "# read_csv. In this case, the separator is a comma (,) so that's what \n",
    "# we'll use.\n",
    "dataset = pd.read_csv(MET_DATA_CSV, sep=',') \n",
    "dataset  # <- this is a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741d297",
   "metadata": {},
   "source": [
    "## 2.3 Accessing and displaying data\n",
    "\n",
    "### 2.3.1 Integer indexing\n",
    "Similar to Python list slices, uses 0-indexed start and end positions to return a subset of the dataframe. With a Padas dataframe, this is done via the `iloc` indexer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35516445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows number 29 to 35\n",
    "int_indexing = dataset.iloc[29 : 35]\n",
    "int_indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66c26f",
   "metadata": {},
   "source": [
    "### 2.3.2 Boolean Series\n",
    "A series is a 1-D array - a boolean series is one that is filled with boolean (i.e., `True` or `False`) values. We can pass boolean series into a Dataframe's `loc` indexer to keep only the values that align with `True`. Different boolean series of the same length can be combined using the following logical operators: `&` (and), `|` (or), `~` (not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all rows from the 'Medieval Art' department\n",
    "medieval_art_bool_series = dataset['Department'] == \"Medieval Art\"\n",
    "dataset.loc[medieval_art_bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rows from the 'Medieval Art' OR 'European Sculpture and Decorative Arts' departments\n",
    "esada_bool_series = dataset['Department'] == \"European Sculpture and Decorative Arts\"\n",
    "both = esada_bool_series | medieval_art_bool_series\n",
    "dataset.loc[both]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863274b",
   "metadata": {},
   "source": [
    "### 2.3.3 Grouping by column name(s)\n",
    "We can also group the data by a list of columns. This returns a Pandas GroupBy object, which contains a dictionary of mappings from each group name to a Series of its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset.groupby(['Object Name', 'Culture'])\n",
    "g = g.size().reset_index(name='Counts')\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347da7b",
   "metadata": {},
   "source": [
    "### 2.3.4 Using `where`\n",
    "Similar to Numpy arrays, Pandas dataframes also make use of the `where` function to conditionally modify its elements based on some criteria. `where` takes a dataframe condition as an argument and returns the modified dataframe - if the condition is fulfilled, it keeps the value of the field, if not, it replaces it with `NaN`. We can use this function to remove objects that do not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.where(g['Counts']>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d66767",
   "metadata": {},
   "source": [
    "As you can see, most of the records are now NaN. We will look at how these types of situations can be fixed in the upcoming cells. Let's  print out the original dataset again so we can use it as a visual reference for the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffb35c4",
   "metadata": {},
   "source": [
    "## 2.4 Data cleaning\n",
    "Before data can be fed into your application, it needs to be verified and checked for consistency. We can see that there are several problems with the dataset right off the bat:\n",
    "1. First row seems to contain garbage: none of the column names match up with the data types, and many are NaN\n",
    "2. It looks like many of the columns are completely empty - they add nothing to the dataset but clutter it\n",
    "3. Too many columns! This depends on what your needs are, but we don't need all of them for this exercise\n",
    "4. Inconsistent formatting in the Dimensions column - makes it difficult to use them downstream\n",
    "5. Mixed datatypes in Year fields\n",
    "\n",
    "Let's address all of these issues one by one\n",
    "\n",
    "### 2.4.1 Deleting rows by index\n",
    "We can get rid of the first row (index 0) by taking a slice of the dataframe beginning at index 1 and going all the way to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2579d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the first row, we can use dataframe slicing to accomplish this:\n",
    "dataset = dataset.iloc[1:]\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2dc44",
   "metadata": {},
   "source": [
    "### 2.4.2 Removing columns\n",
    "\n",
    "Columns can be removed conditionally by checking their contents to see if they meet a certain criteria, or simply by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88098fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all columns that are completely empty\n",
    "dataset = dataset.dropna(how='all', axis=1) # how=all means drop this key if all items are NaN. Axis=1 means work on columns\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf399e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, we can also drop the columns that are irrelevant to our needs\n",
    "exclude_cols = [\n",
    "    \"Country\",\n",
    "    \"Culture\",\n",
    "    \"Is Highlight\",\n",
    "    \"Is Timeline Work\",\n",
    "    \"Object End Date\",\n",
    "    \"Gallery Number\",\n",
    "    \"Period\",\n",
    "    \"Constituent ID\",\n",
    "    \"Artist Role\",\n",
    "    \"Artist Prefix\",\n",
    "    \"Artist Display Name\",\n",
    "    \"Artist Display Bio\",\n",
    "    \"Artist Suffix\",\n",
    "    \"Artist Alpha Sort\",\n",
    "    \"Artist Gender\",\n",
    "    \"Artist Nationality\",\n",
    "    \"Artist ULAN URL\",\n",
    "    \"Artist Wikidata URL\",\n",
    "    \"Credit Line\",\n",
    "    \"Object ID\",\n",
    "    \"Geography Type\",\n",
    "    \"City\",\n",
    "    \"State\",\n",
    "    \"County\",\n",
    "    \"Region\",\n",
    "    \"Classification\",\n",
    "    \"Rights and Reproduction\",\n",
    "    \"Link Resource\",\n",
    "    \"Object Wikidata URL\",\n",
    "    \"Repository\",\n",
    "    \"Tags AAT URL\",\n",
    "    \"Tags Wikidata URL\",\n",
    "    \"Artist Begin Date\",\n",
    "    \"Artist End Date\",\n",
    "    \"Object Date\",\n",
    "]\n",
    "\n",
    "dataset = dataset.drop(exclude_cols, axis=1) # again axis=1 means we work on the columns of this dataframe\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab73dee",
   "metadata": {},
   "source": [
    "### 2.4.3 Removing rows\n",
    "We can also get rid of rows that do not meet certain criteria. For example, given a subset of fields that we deem very important, we can drop all rows are NaN in any of these fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping records (rows)\n",
    "important_cols = ['Tags', 'Dimensions', 'Object Begin Date', 'AccessionYear'] \n",
    "dataset = dataset.dropna(subset=important_cols)\n",
    "dataset\n",
    "\n",
    "# Note that doing this cut the size of our dataset in half! Compare the number of rows in the previous cell to this one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa9969",
   "metadata": {},
   "source": [
    "### 2.4.4 Applying functions to columns\n",
    "\n",
    "We noted earlier that the Dimensions field is a bit messy (inconsistent mixing of imperial and metric units).  We need it to have consistent formatting so that any functions we write later can work with the values without any complicated processing.  \n",
    "\n",
    "Doing the complicated work up front saves a lot of time down the line! Don't worry too much about how exactly this function works**\\*\\***. This is just to show that we can write arbitrarily complex cleaning functions and apply it to a Dataframe's columns. The important thing to note about this function is that if it can't find a suitable dimension to extract for whatever reason, **it will fail, and on failure will return NaN** (not a number).  \n",
    "\n",
    "When this function is applied to the Dimensions column, the column will be left with values that look like:\n",
    "1. a OR\n",
    "2. a, b OR\n",
    "3. a, b, c OR\n",
    "4. NaN\n",
    "\n",
    "where a, b and c are lengths in centimetres and NaN is an indication that the extraction function has failed\n",
    "\n",
    "We can apply this function using the DataFrame's `apply` method. This takes the function to be applied, along with any additional arguments (In addition to the value of the column, of course)\n",
    "\n",
    "\n",
    "_\\*\\* **For those of you who are interested, this function uses [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to find matching substrings of a given input string. You can play around with and [learn regex here](https://regexr.com/)**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2266f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Defining the function that will be used to extract metric dimensions. The first argument to\n",
    "# the function must be the value in the column. \n",
    "def extract_dimensions(dim_str):\n",
    "    dimensions_exp = r\"(?<=\\()( *\\d.+?)(?=cm\\))\"\n",
    "    delimiter_exp = r\"(?:\\d+\\.?\\d* *)([^\\.\\n])(?: *\\d+\\.?\\d* *)?(?:[^\\.\\n] *\\d+\\.?\\d*)?\"\n",
    "    retval = np.nan\n",
    "    try:\n",
    "        dim_str = dim_str.split(\"\\n\")[0]\n",
    "        dimensions = re.search(dimensions_exp, dim_str).group(0)\n",
    "        delimiter = re.search(delimiter_exp, dimensions.strip()).groups()[0]\n",
    "        if not delimiter.isnumeric():\n",
    "            # There are multiple dimensions\n",
    "            retval = ','.join(dimensions.split(delimiter))\n",
    "        else:\n",
    "            retval = dimensions\n",
    "    except AttributeError as e:\n",
    "        pass\n",
    "    finally:\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2aa2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the dimensions of the pieces\n",
    "dataset['Dimensions'] = dataset['Dimensions'].apply(extract_dimensions, args=()) # using the extract_dimensions function we defined earlier\n",
    "\n",
    "# Because extract_dimensions can fail and return NaN, we also need to remove any records where Dimensions is NaN\n",
    "dataset = dataset.dropna(subset=important_cols)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deff332",
   "metadata": {},
   "source": [
    "### 2.4.5 Default values\n",
    "Artists are known to often leave their work untitled. In our dataset, this is not handled very gracefully - the titles of such artworks are simple NaN. Fortunately, we have another way of dealing with missing data: assinging a default value. We can replace any instance of a NaN title with the string \"Untitled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default values. \n",
    "\n",
    "dataset['Title'].fillna('Untitled', inplace=True) # This replaces any instance of NaN in the Title column with Untitled\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf99cdc",
   "metadata": {},
   "source": [
    "### 2.4.6 Data types\n",
    "We change some data types that don't really make sense: `AccessionYear` and `Object Begin Date` were originally loaded in with mixed datatypes (some are strings, some are numbers), which makes it difficult to sort correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b60800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.astype({'AccessionYear': 'int', 'Object Begin Date': 'int'})\n",
    "\n",
    "# Finally, we can sort a dataframe based on the values in a specific column\n",
    "dataset = dataset.sort_values('AccessionYear')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59d5bd",
   "metadata": {},
   "source": [
    "## 2.5 Dataframe interrogation\n",
    "We can now begin to ask some interesting questions about this dataset:\n",
    "1. Which department houses the oldest artwork in the museum? Use `Object Begin Date` for this task.\n",
    "2. What is the proportion of artworks from each department? Display this graphically using [`pd.DataFrame.plot.pie`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.pie.html)\n",
    "3. What is the average area in $cm^2$ of Paintings in the Asian Art department?\n",
    "4. _What is the most common theme across all the paintings? Or, which tag is most common?**\\*\\***_  \n",
    "\n",
    "_**\\*\\* Slightly more difficult, multi-step problem**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c3b46",
   "metadata": {},
   "source": [
    "### 2.5.1 Exercise\n",
    "Oldest artwork in the museum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sorted = ...\n",
    "oldest_record = ... # access the oldest record\n",
    "department_name = ... # get the name of the Department from the record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8aec81",
   "metadata": {},
   "source": [
    "### 2.5.2 Exercise\n",
    "Plot the proportion of artworks from each department as a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "departments = ...\n",
    "departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15506de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame has a built-in plotting function that you can \n",
    "# call like so: \n",
    "departments.plot.pie(y=\"Counts\", \n",
    "                     explode=[0.125 for _ in range(len(departments))], \n",
    "                     labels=departments['Department'], \n",
    "                     legend=None, ylabel=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128179a4",
   "metadata": {},
   "source": [
    "### 2.5.3 Question 3\n",
    "What is the average area in $cm^2$ of Paintings in the Asian wing? Begin by selecting the pieces that belong to the Asian Art department AND are paintings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "department_bool_series = ...\n",
    "painting_bool_series = ...\n",
    "asian_paintings = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ae665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# We can apply a simple area calculation function to each item in the Dimensions column\n",
    "def calc_area(dim_string):\n",
    "    # Note that some paintings are listed as having only 1 dimension - here we can assume that they are \n",
    "    # circular and that the given dimension is its diameter (and probably not radius as that cannot be measured as easily)\n",
    "    dims = dim_string.split(',') # extract dimensions from the string and place them into a list\n",
    "    if len(dims) == 1:\n",
    "        # We have a circular painting, apply the circular area formula\n",
    "        diameter = float(dims[0])  # dimensions are strings, so need to be converted to floats\n",
    "        radius = diameter/2        \n",
    "        return math.pi * radius * radius\n",
    "    elif len(dims) >= 2:\n",
    "        # We have a rectangular painting, use the rectangular area formula\n",
    "        # Some paintings probably have a 3rd dimension to indicate the depth\n",
    "        # of its frame, we can ignore this and only take the first 2 elements\n",
    "        width, height  = float(dims[0]), float(dims[1]) \n",
    "        return width * height\n",
    "    \n",
    "# Apply the function to asian_paintings and assign the result of it to a new column Area \n",
    "asian_paintings['Area']  = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have an Area column that we can reduce to find the average of the paintings in the Asian Art department:\n",
    "average_area = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65c3b85",
   "metadata": {},
   "source": [
    "### 2.5.4 Question 4\n",
    "What is the most common theme across all the paintings? Or, which tag is most common?\n",
    "\n",
    "Note: You may find Python's [string split](https://python-reference.readthedocs.io/en/latest/docs/str/split.html) and [extend](https://python-reference.readthedocs.io/en/latest/docs/list/extend.html) functions very helpful in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab405ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare an empty list to collect all the separated tags in the database\n",
    "tags_list = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Tags series out of the dataset and assign it to a variable\n",
    "# called tags_series\n",
    "tags_series = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7041912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that accepts a Tag string and a list, and updates \n",
    "# the list with the individual tags found in the string.\n",
    "def update_list(t, l):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17762d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply your function update_list to tag_series.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d66cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the tags list into a dataframe called tags_df\n",
    "tags_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by Tag and count the occurences of each tag\n",
    "tags_df = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the list by count to find the one with the tag with the \n",
    "# highest number of occurences\n",
    "tags_df = ...\n",
    "highest_occurenct = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
